{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/ipykernel_launcher.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import gcsfs\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "import cftime\n",
    "import json\n",
    "from dask import array\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 12, 6\n",
    "%config InlineBackend.figure_format = 'retina' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.dashboard.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at:  tcp://10.48.160.10:35845\n",
      "distributed.scheduler - INFO -   dashboard at:                     :8787\n",
      "distributed.scheduler - INFO - Receive client connection: Client-b810573e-6de8-11ea-809d-0a2b65f2e791\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.48.160.10:35845</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/ghall3-pangeo_tests-65pk03ig/proxy/8787/status' target='_blank'>/user/ghall3-pangeo_tests-65pk03ig/proxy/8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.48.160.10:35845' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_kubernetes import KubeCluster\n",
    "\n",
    "cluster = KubeCluster()\n",
    "cluster.adapt(minimum=1, maximum=20, interval='2s')\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_id</th>\n",
       "      <th>institution_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>table_id</th>\n",
       "      <th>variable_id</th>\n",
       "      <th>grid_label</th>\n",
       "      <th>zstore</th>\n",
       "      <th>dcpp_init_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AerChemMIP</td>\n",
       "      <td>BCC</td>\n",
       "      <td>BCC-ESM1</td>\n",
       "      <td>histSST</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>AERmon</td>\n",
       "      <td>mmrbc</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/AerChemMIP/BCC/BCC-ESM1/histSST/r1i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AerChemMIP</td>\n",
       "      <td>BCC</td>\n",
       "      <td>BCC-ESM1</td>\n",
       "      <td>histSST</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>AERmon</td>\n",
       "      <td>mmrdust</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/AerChemMIP/BCC/BCC-ESM1/histSST/r1i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AerChemMIP</td>\n",
       "      <td>BCC</td>\n",
       "      <td>BCC-ESM1</td>\n",
       "      <td>histSST</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>AERmon</td>\n",
       "      <td>mmroa</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/AerChemMIP/BCC/BCC-ESM1/histSST/r1i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AerChemMIP</td>\n",
       "      <td>BCC</td>\n",
       "      <td>BCC-ESM1</td>\n",
       "      <td>histSST</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>AERmon</td>\n",
       "      <td>mmrso4</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/AerChemMIP/BCC/BCC-ESM1/histSST/r1i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AerChemMIP</td>\n",
       "      <td>BCC</td>\n",
       "      <td>BCC-ESM1</td>\n",
       "      <td>histSST</td>\n",
       "      <td>r1i1p1f1</td>\n",
       "      <td>AERmon</td>\n",
       "      <td>mmrss</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/AerChemMIP/BCC/BCC-ESM1/histSST/r1i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activity_id institution_id source_id experiment_id member_id table_id  \\\n",
       "0  AerChemMIP            BCC  BCC-ESM1       histSST  r1i1p1f1   AERmon   \n",
       "1  AerChemMIP            BCC  BCC-ESM1       histSST  r1i1p1f1   AERmon   \n",
       "2  AerChemMIP            BCC  BCC-ESM1       histSST  r1i1p1f1   AERmon   \n",
       "3  AerChemMIP            BCC  BCC-ESM1       histSST  r1i1p1f1   AERmon   \n",
       "4  AerChemMIP            BCC  BCC-ESM1       histSST  r1i1p1f1   AERmon   \n",
       "\n",
       "  variable_id grid_label                                             zstore  \\\n",
       "0       mmrbc         gn  gs://cmip6/AerChemMIP/BCC/BCC-ESM1/histSST/r1i...   \n",
       "1     mmrdust         gn  gs://cmip6/AerChemMIP/BCC/BCC-ESM1/histSST/r1i...   \n",
       "2       mmroa         gn  gs://cmip6/AerChemMIP/BCC/BCC-ESM1/histSST/r1i...   \n",
       "3      mmrso4         gn  gs://cmip6/AerChemMIP/BCC/BCC-ESM1/histSST/r1i...   \n",
       "4       mmrss         gn  gs://cmip6/AerChemMIP/BCC/BCC-ESM1/histSST/r1i...   \n",
       "\n",
       "   dcpp_init_year  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_csv('pangeo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_attrs = json.load(open('file_attrs.txt','r'))\n",
    "# all_attrs = set(['_'.join([a['source_id'],a['member_id'],a['experiment_id'],a['table_id'],a['variable_id']]) for a in file_attrs])\n",
    "\n",
    "# def want(s, m, e, t, v):\n",
    "#     key = s+'_'+m+'_'+e+'_'+t+'_'+v\n",
    "#     return s+'_'+m+'_'+e+'_'+t+'_'+v\n",
    "\n",
    "# dfs = df[df[['source_id','member_id','experiment_id','table_id','variable_id']].apply(lambda x: want(*x) in all_attrs, axis=1)]\n",
    "\n",
    "# pangeo_attrs = set(dfs[['source_id','member_id','experiment_id','table_id','variable_id']].apply(lambda x: want(*x), axis=1).values)\n",
    "# manual_attrs = [a for a in all_attrs if a not in pangeo_attrs]\n",
    "# json.dump(list(pangeo_attrs), open('pangeo_loads.txt', 'w'))\n",
    "# json.dump(list(manual_attrs), open('manual_loads.txt', 'w'))\n",
    "\n",
    "# dfs.to_csv('pangeo_loads.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_srch_data(df, source_id, expt_id):\n",
    "\n",
    "    uri = df[(df.source_id == source_id) &\n",
    "                         (df.experiment_id == expt_id)].zstore.values[0]\n",
    "    \n",
    "    ds = xr.open_zarr(gcs.get_mapper(uri), consolidated=True)\n",
    "    return ds\n",
    "\n",
    "def load_data(series):\n",
    "    ds = xr.open_zarr(gcs.get_mapper(series.zstore), consolidated=True)\n",
    "    return ds\n",
    "\n",
    "def get_dims(ds):\n",
    "    ds_coords = [l for l in list(ds.coords.keys()) if 'bnds' not in l and 'vert' not in l]\n",
    "    dims = [[l for l in ds_coords if 'lat' in l][0], [l for l in ds_coords if 'lon' in l][0]]\n",
    "    lon = ds.coords.get(dims[0])\n",
    "    lat = ds.coords.get(dims[1])\n",
    "    return lat, lon\n",
    "\n",
    "def get_area(ds, df):\n",
    "    var = ds.get(ds.variable_id)\n",
    "    realm = ds.table_id[0].lower()\n",
    "    area = None\n",
    "    lat, lon = get_dims(ds)\n",
    "    dims = [lat.name, lon.name]\n",
    "\n",
    "    df_area = df.query(\"variable_id == 'areacell\"+realm+\"' & source_id == '\"+ds.source_id+\"'\")\n",
    "    if len(df_area.zstore.values) == 0:\n",
    "        if len(lat) > 1000:\n",
    "            area = lat\n",
    "            dims = [\"ncells\"]\n",
    "            total_area = lat.sum()\n",
    "        time, lon, area = np.meshgrid(ds.time, lon, np.cos(lat), indexing='ij')\n",
    "        total_area = area[0,:,:].sum()\n",
    "    else:\n",
    "        ds_area = xr.open_zarr(gcs.get_mapper(df_area.zstore.values[0]), consolidated=True)\n",
    "        area = ds_area.get(\"areacell\"+realm)\n",
    "        total_area = area.sum(dims)\n",
    "\n",
    "    return area, dims, total_area\n",
    "\n",
    "def avg_var(ds, df):\n",
    "    area, dims, total_area = get_area(ds, df)\n",
    "    var = ds.get(ds.variable_id)\n",
    "    \n",
    "    ta_timeseries = (var * area).sum(dim=dims) / total_area\n",
    "    \n",
    "    if isinstance(ta_timeseries, type(None)):\n",
    "        print('failed')\n",
    "    return ta_timeseries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038b965ece6c49a78545bc99ce74afc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=416.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.4:42627', name: 0, memory: 0, processing: 2>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.4:42627\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.5:35315', name: 1, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.5:35315\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Retire worker names (1,)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.48.161.5:35315', name: 1, memory: 5, processing: 0>}\n",
      "distributed.scheduler - INFO - Moving 2 keys to other workers\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.161.5:35315\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.161.5:35315', name: 1, memory: 5, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.161.5:35315\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [1]\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.6:42387', name: 1, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.6:42387\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Retire worker names (0,)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.48.161.4:42627', name: 0, memory: 3, processing: 0>}\n",
      "distributed.scheduler - INFO - Moving 3 keys to other workers\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.161.4:42627\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.161.4:42627', name: 0, memory: 3, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.161.4:42627\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [0]\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.7:43283', name: 2, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.7:43283\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Retire worker names (1,)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.48.161.6:42387', name: 1, memory: 3, processing: 0>}\n",
      "distributed.scheduler - INFO - Moving 2 keys to other workers\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.161.6:42387\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.161.6:42387', name: 1, memory: 3, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.161.6:42387\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [1]\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.8:39669', name: 4, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.8:39669\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.9:38275', name: 3, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.9:38275\n",
      "distributed.core - INFO - Starting established connection\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "distributed.scheduler - INFO - Retire worker names (3,)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.48.161.9:38275', name: 3, memory: 4, processing: 0>}\n",
      "distributed.scheduler - INFO - Moving 1 keys to other workers\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.161.9:38275\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.161.9:38275', name: 3, memory: 4, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.161.9:38275\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [3]\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "distributed.scheduler - INFO - Retire worker names (4,)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.48.161.8:39669', name: 4, memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.161.8:39669\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.161.8:39669', name: 4, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.161.8:39669\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [4]\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.13:33509', name: 6, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.13:33509\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.11:42031', name: 8, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.11:42031\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.14:43699', name: 4, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.14:43699\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.12:41183', name: 5, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.12:41183\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.10:43729', name: 7, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.10:43729\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Retire worker names (2, 5, 6, 7)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.48.161.7:43283', name: 2, memory: 0, processing: 0>, <Worker 'tcp://10.48.161.12:41183', name: 5, memory: 0, processing: 0>, <Worker 'tcp://10.48.161.10:43729', name: 7, memory: 2, processing: 0>, <Worker 'tcp://10.48.161.13:33509', name: 6, memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.161.10:43729\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.161.10:43729', name: 7, memory: 2, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.161.10:43729\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.161.13:33509\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.161.13:33509', name: 6, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.161.13:33509\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.161.12:41183\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.161.12:41183', name: 5, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.161.12:41183\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.161.7:43283\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.161.7:43283', name: 2, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.161.7:43283\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [2, 5, 6, 7]\n",
      "distributed.scheduler - INFO - Retire worker names (4,)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.48.161.14:43699', name: 4, memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.161.14:43699\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.161.14:43699', name: 4, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.161.14:43699\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [4]\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.15:36753', name: 9, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.15:36753\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Retire worker names (9,)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.48.161.15:36753', name: 9, memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.161.15:36753\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.161.15:36753', name: 9, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.161.15:36753\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [9]\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.17:34005', name: 11, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.17:34005\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.16:34285', name: 10, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.16:34285\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.161.19:33189', name: 9, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.161.19:33189\n",
      "distributed.core - INFO - Starting established connection\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 9.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.scheduler - INFO - Retire worker names (10, 11)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.48.161.17:34005', name: 11, memory: 0, processing: 0>, <Worker 'tcp://10.48.161.16:34285', name: 10, memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.161.16:34285\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.161.16:34285', name: 10, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.161.16:34285\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.161.17:34005\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.161.17:34005', name: 11, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.161.17:34005\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [10, 11]\n",
      "distributed.scheduler - INFO - Retire worker names (9,)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.48.161.19:33189', name: 9, memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.161.19:33189\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.161.19:33189', name: 9, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.161.19:33189\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [9]\n"
     ]
    }
   ],
   "source": [
    "for num in tqdm(list(range(len(dfs)))):\n",
    "#     try:\n",
    "    s = dfs.iloc[num]\n",
    "    name = '_'.join([s.source_id, s.experiment_id, s.member_id, s.variable_id])\n",
    "    \n",
    "    if s.table_id == 'Omon':\n",
    "        continue\n",
    "    \n",
    "    if name + '.npy' in os.listdir('data'):\n",
    "        print(name)\n",
    "        continue\n",
    "        \n",
    "    ds = load_data(s)\n",
    "    if len(get_dims(ds)[0]) > 1000:\n",
    "        continue\n",
    "    m = None\n",
    "    m = avg_var(ds, df)\n",
    "\n",
    "    if not isinstance(m, type(None)):\n",
    "#             results[name] = np.array([m.values, np.array([np.datetime64(t) for t in m.time.values])])\n",
    "        np.save('data/'+name, np.array([m.values[:], np.array([np.datetime64(t) for t in m.time.values])[:]]))\n",
    "#     except:\n",
    "#         print(num)\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = []\n",
    "lons = []\n",
    "for r in tqdm(range(len(dfs))):\n",
    "    ds = load_data(dfs.iloc[r])\n",
    "    ds_coords = [l for l in list(ds.coords.keys()) if 'bnds' not in l and 'vert' not in l]\n",
    "    lat_key = [l for l in ds_coords if 'lat' in l]\n",
    "    lon_key = [l for l in ds_coords if 'lon' in l]\n",
    "    lats += [lat_key]\n",
    "    lons += [lon_key]\n",
    "print(set(lats))\n",
    "print(set(lons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentiles(ds, df):\n",
    "    Area = get_area(ds, df)\n",
    "    Var = ds.get(ds.variable_id)\n",
    "    \n",
    "    percentiles = []\n",
    "    \n",
    "    for t in range(len(Var.time)):\n",
    "        area = Area[t]\n",
    "        var = Var[t]\n",
    "        time = Var.time[t]\n",
    "        \n",
    "        weights = np.reshape(area.data, (np.prod(np.shape(var)),1))\n",
    "        vals = np.reshape(var.data, (np.prod(np.shape(var)),1))\n",
    "        weights = np.array(weights[~np.isnan(vals)])\n",
    "        vals = np.array(vals[~np.isnan(vals)])\n",
    "\n",
    "        idx = np.argsort(vals)\n",
    "        vals = np.take_along_axis(vals, idx, axis=0)\n",
    "        sorted_weights = np.array(np.take_along_axis(weights, idx, axis=0))\n",
    "        total = np.nansum(weights)\n",
    "\n",
    "        i = 0\n",
    "        low = 0\n",
    "        N = len(weights)\n",
    "        pcts = np.array([0.05, 0.17, 0.5, 0.83, 0.95])\n",
    "        pct_vals = []\n",
    "        for j in range(0,len(vals)):\n",
    "            low = low + sorted_weights[j]\n",
    "            high = low + sorted_weights[min(j+1,N)]\n",
    "            # If the cumulative weights are nearest the next percentile\n",
    "            # Then mark down the value\n",
    "            if low/total < pcts[i] and high/total >= pcts[i]:\n",
    "                pct_vals += [[pcts[i], vals[j]]]\n",
    "                if i == np.shape(pcts)[0] - 1:\n",
    "                    break\n",
    "                i = i + 1\n",
    "                \n",
    "        percentiles += [time, pct_vals]\n",
    "    \n",
    "    return percentiles\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
