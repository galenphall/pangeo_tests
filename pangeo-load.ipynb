{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/ipykernel_launcher.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import gcsfs\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "import cftime\n",
    "import json\n",
    "from dask import array\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 12, 6\n",
    "%config InlineBackend.figure_format = 'retina' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.dashboard.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at:   tcp://10.48.179.2:37999\n",
      "distributed.scheduler - INFO -   dashboard at:                     :8787\n",
      "distributed.scheduler - INFO - Receive client connection: Client-67a76eee-6eef-11ea-a577-cecc0997d3e4\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.48.179.2:37999</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/ghall3-pangeo_tests-aajlpvy0/proxy/8787/status' target='_blank'>/user/ghall3-pangeo_tests-aajlpvy0/proxy/8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.48.179.2:37999' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.180.46:34285', name: 0, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.180.46:34285\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_kubernetes import KubeCluster\n",
    "\n",
    "cluster = KubeCluster()\n",
    "cluster.adapt(minimum=1, maximum=20, interval='2s')\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_csv('pangeo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_srch_data(df, source_id, expt_id):\n",
    "\n",
    "    uri = df[(df.source_id == source_id) &\n",
    "                         (df.experiment_id == expt_id)].zstore.values[0]\n",
    "    \n",
    "    ds = xr.open_zarr(gcs.get_mapper(uri), consolidated=True)\n",
    "    return ds\n",
    "\n",
    "def load_data(series):\n",
    "    ds = xr.open_zarr(gcs.get_mapper(series.zstore), consolidated=True)\n",
    "    return ds\n",
    "\n",
    "def get_dims(ds):\n",
    "    ds_coords = [l for l in list(ds.coords.keys()) if 'bnds' not in l and 'vert' not in l]\n",
    "    dims = [[l for l in ds_coords if 'lat' in l][0], [l for l in ds_coords if 'lon' in l][0]]\n",
    "    lat = ds.coords.get(dims[0])\n",
    "    lon = ds.coords.get(dims[1])\n",
    "    return lat, lon, dims\n",
    "\n",
    "def get_area(ds, df):\n",
    "    var = ds.get(ds.variable_id)\n",
    "    realm = ds.table_id[0].lower()\n",
    "    lat, lon, dims = get_dims(ds)\n",
    "\n",
    "    df_area = df.query(\"variable_id == 'areacell\"+realm+\"' & source_id == '\"+ds.source_id+\"' & grid_label== '\"+ds.grid_label+\"'\")\n",
    "    if len(df_area.zstore.values) == 0:\n",
    "        if len(lat.data) > 2000:\n",
    "            area = np.cos(lat * np.pi / 180)\n",
    "            dims = [\"ncells\"]\n",
    "            total_area = lat.sum()\n",
    "        elif np.shape(lat) == np.shape(var)[1:]:\n",
    "            area = np.cos(lat.data * np.pi / 180)\n",
    "            total_area = area.sum()\n",
    "            dims = ds.get(dims[0]).dims\n",
    "        else:\n",
    "            time, area, lon = np.meshgrid(ds.time, np.cos(lat.data * np.pi / 180), lon, indexing='ij')\n",
    "            total_area = area[0,:,:].sum()\n",
    "    else:\n",
    "        ds_area = xr.open_zarr(gcs.get_mapper(df_area.zstore.values[0]), consolidated=True)\n",
    "        area = ds_area.get(\"areacell\"+realm)\n",
    "        total_area = area.sum(area.dims)\n",
    "        dims = area.dims\n",
    "\n",
    "    return area, dims, total_area\n",
    "\n",
    "def avg_var(ds, df):\n",
    "    area, dims, total_area = get_area(ds, df)\n",
    "    var = ds.get(ds.variable_id)\n",
    "    \n",
    "    ta_timeseries = (var * area).sum(dim=dims) / total_area\n",
    "    \n",
    "    if isinstance(ta_timeseries, type(None)):\n",
    "        print('failed')\n",
    "    return ta_timeseries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all files available on Pangeo servers\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in tqdm(list(range(len(dfs)))):\n",
    "    s = dfs.iloc[num]\n",
    "    name = '_'.join([s.source_id, s.experiment_id, s.member_id, s.variable_id])\n",
    "    \n",
    "    ds = load_data(s)\n",
    "    df_area = df.query(\"variable_id == 'areacell\"+ds.table_id[0].lower()+\"' & source_id == '\"+ds.source_id+\"' & grid_label== '\"+ds.grid_label+\"'\")\n",
    "    if len(df_area.zstore.values != 0):\n",
    "        continue\n",
    "        \n",
    "    print(str(num)+\" : \"+name)\n",
    "\n",
    "    if ds.experiment_id == 'piControl' or ds.experiment_id == '1pctCO2':\n",
    "        ds = ds.sel(time=slice(ds.time[0], ds.time[min([1799, len(ds.time)-1])]))\n",
    "    elif len(ds.time) > 2400:\n",
    "        ds = ds.sel(time=slice(ds.time[0], ds.time[2399]))\n",
    "    m = avg_var(ds, df)\n",
    "\n",
    "    if not isinstance(m, type(None)):\n",
    "        np.save('data/'+name, np.array([m.values[:], np.array([np.datetime64(t) for t in m.time.values])[:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all files not available on Pangeo's servers\n",
    "========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "manual = json.load(open('manual_loads.txt','r'))\n",
    "allfiles = json.load(open('allfiles.txt','r'))\n",
    "mapping = defaultdict(list)\n",
    "[mapping['_'.join([a.split('/')[9],a.split('/')[11],a.split('/')[10]]+ a.split('/')[12:14])].append(a) for a in allfiles]\n",
    "to_load = [(m, mapping.get(m)) for m in manual]\n",
    "to_load = sorted(to_load, key=lambda x: len(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  CAS-ESM2-0_r1i1p1f1_abrupt-4xCO2_Amon_rsut\n"
     ]
    }
   ],
   "source": [
    "for name, files in to_load[10:]:\n",
    "\n",
    "    ds = xr.open_mfdataset(files, combine='by_coords')\n",
    "    if ds.experiment_id == 'piControl' or ds.experiment_id == '1pctCO2':\n",
    "        ds = ds.sel(time=slice(ds.time[0], ds.time[min([1799, len(ds.time)-1])]))\n",
    "    elif len(ds.time) > 2400:\n",
    "        ds = ds.sel(time=slice(ds.time[0], ds.time[2399]))\n",
    "    \n",
    "    print('loading ',name)\n",
    "     \n",
    "    m = avg_var(ds, df)\n",
    "    m.load()\n",
    "\n",
    "    if not isinstance(m, type(None)):\n",
    "        np.save('esgf_data/'+name, np.array([m.values[:], np.array([np.datetime64(t) for t in m.time.values])[:]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
