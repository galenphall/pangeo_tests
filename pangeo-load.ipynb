{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import gcsfs\n",
    "#from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "import cftime\n",
    "import json\n",
    "from dask import array\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 12, 6\n",
    "%config InlineBackend.figure_format = 'retina' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at:   tcp://10.48.177.9:33833\n",
      "distributed.scheduler - INFO -   dashboard at:                     :8787\n",
      "distributed.scheduler - INFO - Receive client connection: Client-1d6f106c-7f96-11ea-8195-8e6cb5068046\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.48.177.9:33833</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/ghall3-pangeo_tests-m5bmicbe/proxy/8787/status' target='_blank'>/user/ghall3-pangeo_tests-m5bmicbe/proxy/8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.48.177.9:33833' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.178.9:34881', name: 0, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.178.9:34881\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_kubernetes import KubeCluster\n",
    "\n",
    "cluster = KubeCluster()\n",
    "cluster.adapt(minimum=1, maximum=20, interval='2s')\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_csv('records/pangeo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_srch_data(df, source_id, expt_id):\n",
    "\n",
    "    uri = df[(df.source_id == source_id) &\n",
    "                         (df.experiment_id == expt_id)].zstore.values[0]\n",
    "    \n",
    "    ds = xr.open_zarr(gcs.get_mapper(uri), consolidated=True)\n",
    "    return ds\n",
    "\n",
    "def load_data(series):\n",
    "    ds = xr.open_zarr(gcs.get_mapper(series.zstore), consolidated=True)\n",
    "    return ds\n",
    "\n",
    "def get_dims(ds):\n",
    "    ds_coords = [l for l in list(ds.coords.keys()) if 'bnds' not in l and 'vert' not in l]\n",
    "    dims = [[l for l in ds_coords if 'lat' in l][0], [l for l in ds_coords if 'lon' in l][0]]\n",
    "    lat = ds.coords.get(dims[0])\n",
    "    lon = ds.coords.get(dims[1])\n",
    "    return lat, lon, dims\n",
    "\n",
    "def get_area(ds, df):\n",
    "    var = ds.get(ds.variable_id)\n",
    "    realm = ds.table_id[0].lower()\n",
    "    lat, lon, dims = get_dims(ds)\n",
    "\n",
    "#     df_area = df.query(\"variable_id == 'areacell\"+realm+\"' & source_id == '\"+ds.source_id+\"' & grid_label== '\"+ds.grid_label+\"'\")\n",
    "#     if len(df_area.zstore.values) == 0:\n",
    "    if len(lat.data) > 2000:\n",
    "        area = np.cos(lat * np.pi / 180)\n",
    "        dims = [\"ncells\"]\n",
    "        total_area = lat.sum()\n",
    "    elif np.shape(lat) == np.shape(var)[1:]:\n",
    "        area = np.cos(lat.data * np.pi / 180)\n",
    "        total_area = area.sum()\n",
    "        dims = ds.get(dims[0]).dims\n",
    "    else:\n",
    "        time, area, lon = np.meshgrid(ds.time, np.cos(lat.data * np.pi / 180), lon, indexing='ij')\n",
    "        total_area = area[0,:,:].sum()\n",
    "#     else:\n",
    "#         ds_area = xr.open_zarr(gcs.get_mapper(df_area.zstore.values[0]), consolidated=True)\n",
    "#         area = ds_area.get(\"areacell\"+realm)\n",
    "#         total_area = area.sum(area.dims)\n",
    "#         dims = area.dims\n",
    "\n",
    "    return area, dims, total_area\n",
    "\n",
    "def avg_var(ds, df):\n",
    "    area, dims, total_area = get_area(ds, df)\n",
    "    var = ds.get(ds.variable_id)\n",
    "    \n",
    "    ta_timeseries = (var * area).sum(dim=dims) / total_area\n",
    "    \n",
    "    if isinstance(ta_timeseries, type(None)):\n",
    "        print('failed')\n",
    "    return ta_timeseries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all files available on Pangeo servers\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/ipykernel_launcher.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d4cb4a13534c9b99af1ff89f6084f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=161.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNRM-ESM2-1_piControl_r1i1p1f2_tas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.48.178.10:33907', name: 1, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.48.178.10:33907\n",
      "distributed.core - INFO - Starting established connection\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "distributed.scheduler - INFO - Retire worker names (0,)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.48.178.9:34881', name: 0, memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.48.178.9:34881\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.48.178.9:34881', name: 0, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.48.178.9:34881\n"
     ]
    }
   ],
   "source": [
    "for num in tqdm(list(range(len(a_tas)))[10:]):\n",
    "    s = a_tas.iloc[num]\n",
    "    name = '_'.join([s.source_id, s.experiment_id, s.member_id, s.variable_id])\n",
    "    \n",
    "    ds = load_data(s)\n",
    "    print(name)\n",
    "#     df_area = df.query(\"variable_id == 'areacell\"+ds.table_id[0].lower()+\"' & source_id == '\"+ds.source_id+\"' & grid_label== '\"+ds.grid_label+\"'\")\n",
    "#     if len(df_area.zstore.values != 0):\n",
    "#         continue\n",
    "        \n",
    "#     print(str(num)+\" : \"+name)\n",
    "\n",
    "#     if ds.experiment_id == 'piControl' or ds.experiment_id == '1pctCO2':\n",
    "#         ds = ds.sel(time=slice(ds.time[0], ds.time[min([1799, len(ds.time)-1])]))\n",
    "#     elif len(ds.time) > 2400:\n",
    "#         ds = ds.sel(time=slice(ds.time[0], ds.time[2399]))\n",
    "    m = avg_var(ds, df)\n",
    "\n",
    "    if not isinstance(m, type(None)):\n",
    "        np.save('pangeo_data/'+name, np.array([m.values[:], np.array([np.datetime64(t) for t in m.time.values])[:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all files not available on Pangeo's servers\n",
    "========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "manual = json.load(open('manual_loads.txt','r'))\n",
    "allfiles = json.load(open('allfiles.txt','r'))\n",
    "mapping = defaultdict(list)\n",
    "[mapping['_'.join([a.split('/')[9],a.split('/')[11],a.split('/')[10]]+ a.split('/')[12:14])].append(a) for a in allfiles]\n",
    "to_load = [(m, mapping.get(m)) for m in manual]\n",
    "to_load = sorted(to_load, key=lambda x: len(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all info I need to download these by hand.\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = pd.read_csv('saved-data.csv')\n",
    "failed = []\n",
    "esgf = pd.DataFrame([m.split('_') for m in manual])\n",
    "esgf = esgf.rename(columns={0:'source_id',1:'member_id',2:'experiment_id',3:'table_id',4:'variable_id'})\n",
    "esgf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge downloaded mlotst files\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('wgets/abrupt-4xCO2-mlotst')\n",
    "files = pd.DataFrame([m.replace('.nc','').split('_') + [m] for m in files])\n",
    "files = files.rename(columns={0:\"variable_id\",1:\"table_id\",2:\"source_id\",3:\"experiment_id\",4:\"member_id\",5:\"grid_label\",6:\"time_range\",7:\"file_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "failed = []\n",
    "for key, group in files.groupby(['experiment_id','variable_id','source_id','member_id']):\n",
    "    group = group.sort_values(by=['time_range'])\n",
    "    \n",
    "    merged = None\n",
    "    t = False\n",
    "    \n",
    "    try:\n",
    "        for f in group['file_name']:\n",
    "                ds = xr.open_dataset('wgets/abrupt-4xCO2-mlotst/'+f)\n",
    "                m = avg_var(ds, df)\n",
    "                if not t:\n",
    "                    merged = m\n",
    "                else:\n",
    "                    merged = xr.concat([merged, m],\"time\")\n",
    "\n",
    "        if not isinstance(merged, type(None)):\n",
    "            series = group.iloc[0]\n",
    "            fname = '_'.join([series.source_id, series.experiment_id, series.member_id, series.variable_id])\n",
    "            saved = pd.concat([saved, group])\n",
    "            np.save('manual_data/'+fname, np.array([m.values[:], np.array([np.datetime64(t) for t in m.time.values])[:]]))\n",
    "    except OSError:\n",
    "        print(\"failed on\",f)\n",
    "        failed.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing files in cloud\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_csv('saved-data.csv')\n",
    "\n",
    "def find(**keys):\n",
    "    ndfs = dfs.copy()\n",
    "    for k,v in keys.items():\n",
    "        ndfs = ndfs[ndfs[k] == v]\n",
    "    return ndfs\n",
    "\n",
    "def load(ndfs=None, **keys):\n",
    "    if isinstance(ndfs, type(None)):\n",
    "        ndfs = find(**keys)\n",
    "    ns = dict([(f, np.load(f, allow_pickle = True)) for f in ndfs.file_name])\n",
    "    return ns\n",
    "\n",
    "ts_files = find(variable_id = \"ts\")\n",
    "bad_sources = []\n",
    "for file, d in ts_files.groupby('file_name'):\n",
    "    ts = list(load(file_name=file).values())[0]\n",
    "    if np.mean(ts[0]) < 250:\n",
    "        bad_sources += [(d.source_id.values[0],d.experiment_id.values[0], 'ts')]\n",
    "                \n",
    "v = {'piControl':['tos','ts','mlotst'],\n",
    "    '1pctCO2':['tos','ts','rtmt','mlotst'],\n",
    "    'abrupt-4xCO2':['tos','ts','mlotst']}\n",
    "incompletes = []\n",
    "for keys, group in dfs.groupby(['source_id','experiment_id']):\n",
    "    has_all = True\n",
    "    source = keys[0]\n",
    "    expt = keys[1]\n",
    "    s = list(group[group.experiment_id == expt].variable_id.values)\n",
    "    for x in v[expt]:\n",
    "        if x not in s:\n",
    "            incompletes += [(*keys, x)]\n",
    "redownload = incompletes + bad_sources\n",
    "redownload2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = pd.DataFrame()\n",
    "for r in redownload:\n",
    "    l = df[(df.source_id == r[0]) & (df.experiment_id == r[1]) & (df.variable_id == r[2]) & (df.grid_label == 'gn')]\n",
    "    if len(l.zstore.values) > 0:\n",
    "        check = pd.concat([check, l])\n",
    "    else:\n",
    "        redownload2 += [r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas = df[df.variable_id == \"tas\"]\n",
    "taspi = tas[tas.experiment_id == \"piControl\"]\n",
    "tasppt = tas[tas.experiment_id == \"1pctCO2\"]\n",
    "tasab = tas[tas.experiment_id == \"abrupt-4xCO2\"]\n",
    "ks = [set([k for k,g in i.groupby(['source_id'])]) for i in [taspi, tasppt, tasab]]\n",
    "k = ks[0] & ks[1] & ks[2]\n",
    "def check_sc(row):\n",
    "    return (row in k)\n",
    "a_tas = pd.concat([taspi, tasppt, tasab])\n",
    "a_tas = a_tas[a_tas.source_id.apply(check_sc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
