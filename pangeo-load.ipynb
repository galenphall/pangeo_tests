{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ejhall2/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import gcsfs\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "import cftime\n",
    "import json\n",
    "from dask import array\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 12, 6\n",
    "%config InlineBackend.figure_format = 'retina' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.dashboard.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at:    tcp://10.49.57.7:43209\n",
      "distributed.scheduler - INFO -   dashboard at:                     :8787\n",
      "distributed.scheduler - INFO - Receive client connection: Client-a3dfcb94-74a3-11ea-803e-62421ff4bad5\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.49.57.7:43209</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/ghall3-pangeo_tests-ed6xj3cg/proxy/8787/status' target='_blank'>/user/ghall3-pangeo_tests-ed6xj3cg/proxy/8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.49.57.7:43209' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_kubernetes import KubeCluster\n",
    "\n",
    "cluster = KubeCluster()\n",
    "cluster.adapt(minimum=1, maximum=20, interval='2s')\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_csv('pangeo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_srch_data(df, source_id, expt_id):\n",
    "\n",
    "    uri = df[(df.source_id == source_id) &\n",
    "                         (df.experiment_id == expt_id)].zstore.values[0]\n",
    "    \n",
    "    ds = xr.open_zarr(gcs.get_mapper(uri), consolidated=True)\n",
    "    return ds\n",
    "\n",
    "def load_data(series):\n",
    "    ds = xr.open_zarr(gcs.get_mapper(series.zstore), consolidated=True)\n",
    "    return ds\n",
    "\n",
    "def get_dims(ds):\n",
    "    ds_coords = [l for l in list(ds.coords.keys()) if 'bnds' not in l and 'vert' not in l]\n",
    "    dims = [[l for l in ds_coords if 'lat' in l][0], [l for l in ds_coords if 'lon' in l][0]]\n",
    "    lat = ds.coords.get(dims[0])\n",
    "    lon = ds.coords.get(dims[1])\n",
    "    return lat, lon, dims\n",
    "\n",
    "def get_area(ds, df):\n",
    "    var = ds.get(ds.variable_id)\n",
    "    realm = ds.table_id[0].lower()\n",
    "    lat, lon, dims = get_dims(ds)\n",
    "\n",
    "    df_area = df.query(\"variable_id == 'areacell\"+realm+\"' & source_id == '\"+ds.source_id+\"' & grid_label== '\"+ds.grid_label+\"'\")\n",
    "    if len(df_area.zstore.values) == 0:\n",
    "        if len(lat.data) > 2000:\n",
    "            area = np.cos(lat * np.pi / 180)\n",
    "            dims = [\"ncells\"]\n",
    "            total_area = lat.sum()\n",
    "        elif np.shape(lat) == np.shape(var)[1:]:\n",
    "            area = np.cos(lat.data * np.pi / 180)\n",
    "            total_area = area.sum()\n",
    "            dims = ds.get(dims[0]).dims\n",
    "        else:\n",
    "            time, area, lon = np.meshgrid(ds.time, np.cos(lat.data * np.pi / 180), lon, indexing='ij')\n",
    "            total_area = area[0,:,:].sum()\n",
    "    else:\n",
    "        ds_area = xr.open_zarr(gcs.get_mapper(df_area.zstore.values[0]), consolidated=True)\n",
    "        area = ds_area.get(\"areacell\"+realm)\n",
    "        total_area = area.sum(area.dims)\n",
    "        dims = area.dims\n",
    "\n",
    "    return area, dims, total_area\n",
    "\n",
    "def avg_var(ds, df):\n",
    "    area, dims, total_area = get_area(ds, df)\n",
    "    var = ds.get(ds.variable_id)\n",
    "    \n",
    "    ta_timeseries = (var * area).sum(dim=dims) / total_area\n",
    "    \n",
    "    if isinstance(ta_timeseries, type(None)):\n",
    "        print('failed')\n",
    "    return ta_timeseries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all files available on Pangeo servers\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd19c8ac3f34978b971c5d40c833432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.4:39585', name: 1, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.4:39585\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.6:38473', name: 2, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.6:38473\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.7:38935', name: 3, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.7:38935\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Retire worker names (2,)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.49.58.6:38473', name: 2, memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.6:38473\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.6:38473', name: 2, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.6:38473\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [2]\n",
      "distributed.scheduler - INFO - Retire worker names (3,)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.49.58.7:38935', name: 3, memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.7:38935\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.7:38935', name: 3, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.7:38935\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [3]\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.4:39585', name: 1, memory: 1, processing: 2>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.4:39585\n",
      "distributed.scheduler - INFO - Retire worker names (1,)\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [1]\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.8:37753', name: 3, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.8:37753\n",
      "distributed.core - INFO - Starting established connection\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.9:41687', name: 7, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.9:41687\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.12:46515', name: 5, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.12:46515\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.11:38427', name: 6, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.11:38427\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.10:45597', name: 4, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.10:45597\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.14:46319', name: 8, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.14:46319\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.13:35355', name: 9, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.13:35355\n",
      "distributed.core - INFO - Starting established connection\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "distributed.scheduler - INFO - Retire worker names (10,)\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [10]\n",
      "distributed.scheduler - INFO - Retire worker names (0, 8, 9)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.49.58.2:43499', name: 0, memory: 0, processing: 0>, <Worker 'tcp://10.49.58.13:35355', name: 9, memory: 0, processing: 0>, <Worker 'tcp://10.49.58.14:46319', name: 8, memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.2:43499\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.2:43499', name: 0, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.2:43499\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.13:35355\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.13:35355', name: 9, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.13:35355\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.14:46319\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.14:46319', name: 8, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.14:46319\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [0, 8, 9]\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.15:39109', name: 10, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.15:39109\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Retire worker names (10, 5)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.49.58.12:46515', name: 5, memory: 0, processing: 0>, <Worker 'tcp://10.49.58.15:39109', name: 10, memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.12:46515\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.12:46515', name: 5, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.12:46515\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.15:39109\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.15:39109', name: 10, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.15:39109\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [10, 5]\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "distributed.scheduler - INFO - Retire worker names (3,)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.49.58.8:37753', name: 3, memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.8:37753\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.8:37753', name: 3, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.8:37753\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [3]\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.17:36103', name: 10, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.17:36103\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.16:44165', name: 12, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.16:44165\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.18:45923', name: 11, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.18:45923\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Retire worker names (10, 11, 12, 7)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.49.58.18:45923', name: 11, memory: 0, processing: 0>, <Worker 'tcp://10.49.58.16:44165', name: 12, memory: 0, processing: 0>, <Worker 'tcp://10.49.58.17:36103', name: 10, memory: 0, processing: 0>, <Worker 'tcp://10.49.58.9:41687', name: 7, memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.9:41687\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.9:41687', name: 7, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.9:41687\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.16:44165\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.16:44165', name: 12, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.16:44165\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.17:36103\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.17:36103', name: 10, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.17:36103\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.18:45923\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.18:45923', name: 11, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.18:45923\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [10, 11, 12, 7]\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/xarray/coding/times.py:426: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/numpy/core/_asarray.py:85: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.19:34807', name: 12, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.19:34807\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.21:34491', name: 13, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.21:34491\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://10.49.58.20:37291', name: 14, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.49.58.20:37291\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Retire worker names (12, 13, 14)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.49.58.21:34491', name: 13, memory: 0, processing: 0>, <Worker 'tcp://10.49.58.19:34807', name: 12, memory: 0, processing: 0>, <Worker 'tcp://10.49.58.20:37291', name: 14, memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.19:34807\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.19:34807', name: 12, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.19:34807\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.20:37291\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.20:37291', name: 14, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.20:37291\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.49.58.21:34491\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://10.49.58.21:34491', name: 13, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://10.49.58.21:34491\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [12, 13, 14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for num in tqdm(list(range(len(check)))):\n",
    "    s = check.iloc[num]\n",
    "    name = '_'.join([s.source_id, s.experiment_id, s.member_id, s.variable_id])\n",
    "    \n",
    "    ds = load_data(s)\n",
    "#     df_area = df.query(\"variable_id == 'areacell\"+ds.table_id[0].lower()+\"' & source_id == '\"+ds.source_id+\"' & grid_label== '\"+ds.grid_label+\"'\")\n",
    "#     if len(df_area.zstore.values != 0):\n",
    "#         continue\n",
    "        \n",
    "#     print(str(num)+\" : \"+name)\n",
    "\n",
    "#     if ds.experiment_id == 'piControl' or ds.experiment_id == '1pctCO2':\n",
    "#         ds = ds.sel(time=slice(ds.time[0], ds.time[min([1799, len(ds.time)-1])]))\n",
    "#     elif len(ds.time) > 2400:\n",
    "#         ds = ds.sel(time=slice(ds.time[0], ds.time[2399]))\n",
    "    m = avg_var(ds, df)\n",
    "\n",
    "    if not isinstance(m, type(None)):\n",
    "        np.save('pangeo_data/'+name, np.array([m.values[:], np.array([np.datetime64(t) for t in m.time.values])[:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all files not available on Pangeo's servers\n",
    "========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "manual = json.load(open('manual_loads.txt','r'))\n",
    "allfiles = json.load(open('allfiles.txt','r'))\n",
    "mapping = defaultdict(list)\n",
    "[mapping['_'.join([a.split('/')[9],a.split('/')[11],a.split('/')[10]]+ a.split('/')[12:14])].append(a) for a in allfiles]\n",
    "to_load = [(m, mapping.get(m)) for m in manual]\n",
    "to_load = sorted(to_load, key=lambda x: len(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all info I need to download these by hand.\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = pd.read_csv('saved-data.csv')\n",
    "failed = []\n",
    "esgf = pd.DataFrame([m.split('_') for m in manual])\n",
    "esgf = esgf.rename(columns={0:'source_id',1:'member_id',2:'experiment_id',3:'table_id',4:'variable_id'})\n",
    "esgf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge downloaded mlotst files\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('wgets/abrupt-4xCO2-mlotst')\n",
    "files = pd.DataFrame([m.replace('.nc','').split('_') + [m] for m in files])\n",
    "files = files.rename(columns={0:\"variable_id\",1:\"table_id\",2:\"source_id\",3:\"experiment_id\",4:\"member_id\",5:\"grid_label\",6:\"time_range\",7:\"file_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "failed = []\n",
    "for key, group in files.groupby(['experiment_id','variable_id','source_id','member_id']):\n",
    "    group = group.sort_values(by=['time_range'])\n",
    "    \n",
    "    merged = None\n",
    "    t = False\n",
    "    \n",
    "    try:\n",
    "        for f in group['file_name']:\n",
    "                ds = xr.open_dataset('wgets/abrupt-4xCO2-mlotst/'+f)\n",
    "                m = avg_var(ds, df)\n",
    "                if not t:\n",
    "                    merged = m\n",
    "                else:\n",
    "                    merged = xr.concat([merged, m],\"time\")\n",
    "\n",
    "        if not isinstance(merged, type(None)):\n",
    "            series = group.iloc[0]\n",
    "            fname = '_'.join([series.source_id, series.experiment_id, series.member_id, series.variable_id])\n",
    "            saved = pd.concat([saved, group])\n",
    "            np.save('manual_data/'+fname, np.array([m.values[:], np.array([np.datetime64(t) for t in m.time.values])[:]]))\n",
    "    except OSError:\n",
    "        print(\"failed on\",f)\n",
    "        failed.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing files in cloud\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_csv('saved-data.csv')\n",
    "\n",
    "def find(**keys):\n",
    "    ndfs = dfs.copy()\n",
    "    for k,v in keys.items():\n",
    "        ndfs = ndfs[ndfs[k] == v]\n",
    "    return ndfs\n",
    "\n",
    "def load(ndfs=None, **keys):\n",
    "    if isinstance(ndfs, type(None)):\n",
    "        ndfs = find(**keys)\n",
    "    ns = dict([(f, np.load(f, allow_pickle = True)) for f in ndfs.file_name])\n",
    "    return ns\n",
    "\n",
    "ts_files = find(variable_id = \"ts\")\n",
    "bad_sources = []\n",
    "for file, d in ts_files.groupby('file_name'):\n",
    "    ts = list(load(file_name=file).values())[0]\n",
    "    if np.mean(ts[0]) < 250:\n",
    "        bad_sources += [(d.source_id.values[0],d.experiment_id.values[0], 'ts')]\n",
    "                \n",
    "v = {'piControl':['tos','ts','mlotst'],\n",
    "    '1pctCO2':['tos','ts','rtmt','mlotst'],\n",
    "    'abrupt-4xCO2':['tos','ts','mlotst']}\n",
    "incompletes = []\n",
    "for keys, group in dfs.groupby(['source_id','experiment_id']):\n",
    "    has_all = True\n",
    "    source = keys[0]\n",
    "    expt = keys[1]\n",
    "    s = list(group[group.experiment_id == expt].variable_id.values)\n",
    "    for x in v[expt]:\n",
    "        if x not in s:\n",
    "            incompletes += [(*keys, x)]\n",
    "redownload = incompletes + bad_sources\n",
    "redownload2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = pd.DataFrame()\n",
    "for r in redownload:\n",
    "    l = df[(df.source_id == r[0]) & (df.experiment_id == r[1]) & (df.variable_id == r[2]) & (df.grid_label == 'gn')]\n",
    "    if len(l.zstore.values) > 0:\n",
    "        check = pd.concat([check, l])\n",
    "    else:\n",
    "        redownload2 += [r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(redownload2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(check.zstore.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(redownload2).to_csv('redownload.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
